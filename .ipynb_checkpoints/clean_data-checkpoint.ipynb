{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "The dataset was originally in two dataframes: transaction table and identity table. Transaction table is about the transaction made including time, place, card info, product/service etc at the transaction. <br>\n",
    "Identity table is network and digital info associated with transactions.<br>\n",
    "These two tables are merged by the unique key 'TransactionID' and we use this merged data for training and testing.<br>\n",
    "#### Procedures\n",
    "1. Import and split the dataset, check data types\n",
    "2. Impute missing *numeric* values<br>\n",
    "    method 1. Impute missing *numeric* values with median<br>\n",
    "    method 2. Impute missing *numeric* values with Bayesian Ridge Regression from sklearn.linear_model package\n",
    "3. Impute missing *categorical* values with modes.\n",
    "4. Merge the two datasets and convert categorical variables into dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ipynb.fs.full.helper_functions import * # Custom function to create dummy variables\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# Make better use of Jupyter Notebook cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import train and test set and split, and check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "train_identity = pd.read_csv('ieee-fraud-detection/train_identity.csv')\n",
    "train_transaction = pd.read_csv('ieee-fraud-detection/train_transaction.csv')\n",
    "\n",
    "df = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "X, y = df.drop('isFraud', axis=1), df.isFraud\n",
    "\n",
    "X.loc[:, 'TransactionID'] = X.loc[:, 'TransactionID'].astype('object')\n",
    "X_train.loc[:, 'card1':'addr2'] = X_train.loc[:, 'card1':'addr2'].astype('object')\n",
    "X.loc[:, 'id_12':'id_38'] = X.loc[:, 'id_12':'id_38'].astype('object')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, stratify = y, random_state = 1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, stratify = y_test, random_state = 2)\n",
    "\n",
    "# pd.to_pickle(X_train, 'data/X_train.pkl')\n",
    "# pd.to_pickle(X_val, 'data/X_val.pkl')\n",
    "# pd.to_pickle(X_test, 'data/X_test.pkl')\n",
    "# pd.to_pickle(y_train, 'data/y_train.pkl')\n",
    "# pd.to_pickle(y_val, 'data/y_val.pkl')\n",
    "# pd.to_pickle(y_test, 'data/y_test.pkl')\n",
    "\n",
    "print(f\"Num of train set = {len(X_train)}\")\n",
    "print(f\"Num of test set(including validation) = {len(X_test)}\")\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from the saved pickle from the second time running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = pd.read_pickle('data/X_train.pkl'), pd.read_pickle('data/X_val.pkl'), pd.read_pickle('data/X_test.pkl'), pd.read_pickle('data/y_train.pkl'), pd.read_pickle('data/y_val.pkl'), pd.read_pickle('data/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Impute missing numeric values on training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric = X_train.loc[:, X_train.dtypes != 'object']\n",
    "X_val_numeric = X_val.loc[:, X_val.dtypes != 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-1) Impute numeric values with median for training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_median = X_train_numeric.median()\n",
    "imp_num_median_train = X_train_numeric.fillna(imputer_median)\n",
    "imp_num_median_val = X_val_numeric.fillna(imputer_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2) Or impute numeric values with bayesian ridge regression (takes about 11 hours to fit the model and about 1 hour to transform on \"2.5 GHz Quad-Core Intel Core i7   16 GB 1600 MHz DDR3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "imputer_bayesian = IterativeImputer(BayesianRidge())\n",
    "\n",
    "%time imputer_bayesian.fit(X_train_numeric)\n",
    "print(f\"Imputer has been fit.\")\n",
    "pd.to_pickle(imputer_bayesian, 'imputers/imputer_bayesian_train.pkl')\n",
    "\n",
    "%time imp_num_bayesian_train = imputer_bayesian.transform(X_train_numeric)\n",
    "imp_num_bayesian_train = pd.DataFrame(imp_num_bayesian_train, columns = X_train_numeric.columns)\n",
    "\n",
    "X_val_numeric = X_val.loc[:, X_val.dtypes != 'object']\n",
    "imp_num_bayesian_val = pd.DataFrame(imputer_bayesian.transform(X_val_numeric), columns = X_val_numeric.columns)\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Impute missing categorical values on training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = X_train.loc[:, X_train.dtypes == 'object']\n",
    "X_val_cat = X_val.loc[:, X_val.dtypes == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mode = X_train_cat.mode(dropna=True).iloc[0,:]\n",
    "imp_cat_mode_train = X_train_cat.fillna(imputer_mode).astype('object')\n",
    "imp_cat_mode_val = X_val_cat.fillna(imputer_mode).astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Merge the numeric columns and categorical columns and convert the categorical columns into indicator variables using a custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = pd.concat([imp_num_median_train.reset_index(drop=True), imp_cat_mode_train.reset_index(drop=True)], axis = 1)\n",
    "merged_val = pd.concat([imp_num_median_val.reset_index(drop=True), imp_cat_mode_val.reset_index(drop=True)], axis = 1)\n",
    "\n",
    "imp_train = create_dummies(merged_train, merged_train.loc[:, merged_train.dtypes != 'object'].columns)\n",
    "imp_val = create_dummies(merged_val, merged_val.loc[:, merged_val.dtypes != 'object'].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Save the training set and validation set for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(imp_train, 'data/imp_train.pkl')\n",
    "pd.to_pickle(imp_val, 'data/imp_val.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Normally, we would combine training set and validation set to evaluate test set, but due to calibration and xgboost modeling process, we just impute test data with only training set's medians and modes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_numeric = X_test.loc[:, X_test.dtypes != 'object']\n",
    "imp_num_median_test = X_test_numeric.fillna(imputer_median)\n",
    "X_test_cat = X_test.loc[:, X_test.dtypes == 'object']\n",
    "imp_cat_mode_test = X_test_cat.fillna(imputer_mode).astype('object')\n",
    "merged_test = pd.concat([imp_num_median_test.reset_index(drop=True), imp_cat_mode_test.reset_index(drop=True)], axis = 1)\n",
    "imp_test = create_dummies(merged_test, merged_test.loc[:, merged_test.dtypes != 'object'].columns)\n",
    "pd.to_pickle(imp_test, 'data/imp_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same process(step.2 to 5) with training + validation set and the test set\n",
    "# Combine the training set and test set and impute the means and the modes. Then, apply the same imputed values to the test set.\n",
    "\"\"\"\n",
    "\n",
    "X_train_val = pd.concat([X_train.reset_index(drop=True), X_val.reset_index(drop=True)], axis = 0).reset_index(drop=True)\n",
    "\n",
    "X_train_val_numeric = X_train_val.loc[:, X_train_val.dtypes != 'object']\n",
    "X_test_numeric = X_test.loc[:, X_test.dtypes != 'object']\n",
    "\n",
    "imputer_median = X_train_val_numeric.median()\n",
    "imp_num_median_train_val = X_train_val_numeric.fillna(imputer_median)\n",
    "imp_num_median_test = X_test_numeric.fillna(imputer_median)\n",
    "\n",
    "X_train_val_cat = X_train_val.loc[:, X_train_val.dtypes == 'object']\n",
    "X_test_cat = X_test.loc[:, X_test.dtypes == 'object']\n",
    "\n",
    "imputer_mode = X_train_val_cat.mode(dropna=True).iloc[0,:]\n",
    "imp_cat_mode_train_val = X_train_val_cat.fillna(imputer_mode).astype('object')\n",
    "imp_cat_mode_test = X_test_cat.fillna(imputer_mode).astype('object')\n",
    "\n",
    "merged_train_val = pd.concat([imp_num_median_train_val.reset_index(drop=True), imp_cat_mode_train_val.reset_index(drop=True)], axis = 1)\n",
    "merged_test = pd.concat([imp_num_median_test.reset_index(drop=True), imp_cat_mode_test.reset_index(drop=True)], axis = 1)\n",
    "\n",
    "imp_train_val = create_dummies(merged_train_val, merged_train_val.loc[:, merged_train_val.dtypes != 'object'].columns)\n",
    "imp_test = create_dummies(merged_test, merged_test.loc[:, merged_test.dtypes != 'object'].columns)\n",
    "\n",
    "# pd.to_pickle(imp_train_val, 'data/imputation_train+val/imp_train_val.pkl')\n",
    "# pd.to_pickle(imp_test, 'data/imputation_train+val/imp_test.pkl')\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
