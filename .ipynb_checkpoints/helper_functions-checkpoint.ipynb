{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies(df, cols):\n",
    "    \"\"\"\n",
    "    Create dummies for the cateogorical variables of the ieee-fraud-detection training set.\n",
    "    \n",
    "    The categorical variables to be turned into dummies are\n",
    "    ProductCD, card3-6, addr2, P_emaildomain, R_emaildomain, M1,2,3,4,6,7,9, id_12,15,16,28,29,31,35,36,37,38\n",
    "    \n",
    "    Note: These are not all categorical/indicator variables in the training set. \n",
    "          They were manually selected based on the correlation with the target.\n",
    "    \n",
    "    args:\n",
    "    df(pd.DataFrame): ieee-fraud-detection training/validation/test set\n",
    "    cols: numeric columns that would like to be merged with the dummies\n",
    "    \n",
    "    returns:\n",
    "    df(pd.DataFrame): dataframe with dummies and the input columns.\n",
    "    \"\"\"\n",
    "\n",
    "    product_dummies = pd.get_dummies(df.ProductCD)\n",
    "    card4_dummies = pd.get_dummies(df.card4)\n",
    "    \n",
    "    card3_dummy = df.card3.map(lambda x: 1 if x == 150.0 else (x if pd.isnull(x) else 0)) # 88% has '150.0'    \n",
    "    card5_dummy = df.card5.map(lambda x: 1 if x == 226.0 else (x if pd.isnull(x) else 0)) # 50% has '226.0'\n",
    "    card6_debit = df.card6.map(lambda x: 1 if x == 'debit' else (x if pd.isnull(x) else 0))\n",
    "    card6_credit = df.card6.map(lambda x: 1 if x == 'credit' else (x if pd.isnull(x) else 0))\n",
    "    card6_others = df.card6.map(lambda x: 1 if (x != 'debit') & (x != 'credit') else (x if pd.isnull(x) else 0))\n",
    "\n",
    "    card_dummies = pd.concat([card3_dummy, card5_dummy, card6_debit, card6_credit, card6_others], axis = 1)\n",
    "    card_dummies.columns = ['card3_150', 'card5_226', 'card6_debit', 'card6_credit', 'card6_neither']\n",
    "\n",
    "    addr2_dummy = df.addr2.map(lambda x: 1 if x == 87.0 else (x if pd.isnull(x) else 0))# 88% has '87'\n",
    "\n",
    "    P_emaildomain = df.P_emaildomain.map(lambda x: x.split('.')[0] if type(x) == str else x)\n",
    "    P_emaildomain_dummies_columns = ['gmail','yahoo','hotmail','anonymous','aol','comcast','icloud','outlook','msn','others']\n",
    "    P_emaildomain_dummies = pd.DataFrame()\n",
    "    for colname in P_emaildomain_dummies_columns:\n",
    "        if colname == 'others':\n",
    "            col = P_emaildomain.map(lambda x: 1 if x not in {'gmail','yahoo','hotmail','anonymous','aol','comcast','icloud','outlook','msn'} else (x if pd.isnull(x) else 0))\n",
    "            P_emaildomain_dummies = pd.concat([P_emaildomain_dummies.reset_index(drop=True), col.reset_index(drop=True)], axis = 1)\n",
    "            continue\n",
    "        P_emaildomain_dummies = pd.concat([P_emaildomain_dummies, \n",
    "                                           P_emaildomain.map(lambda x: 1 if x == colname else (x if pd.isnull(x) else 0))], \n",
    "                                          ignore_index=True, axis = 1)\n",
    "    P_emaildomain_dummies.columns = ['P_' + col for col in P_emaildomain_dummies_columns]\n",
    "\n",
    "    \n",
    "    R_emaildomain = df.R_emaildomain.map(lambda x: x.split('.')[0] if type(x) == str else x)\n",
    "    R_emaildomain_dummies_columns = ['gmail','yahoo','hotmail','anonymous','aol','comcast','icloud','outlook','msn','others']\n",
    "    R_emaildomain_dummies = pd.DataFrame()\n",
    "    for colname in R_emaildomain_dummies_columns:\n",
    "        if colname == 'others':\n",
    "            col = R_emaildomain.map(lambda x: 1 if x not in {'gmail','yahoo','hotmail','anonymous','aol','comcast','icloud','outlook','msn'} else (x if pd.isnull(x) else 0))\n",
    "            R_emaildomain_dummies = pd.concat([R_emaildomain_dummies.reset_index(drop=True), col.reset_index(drop=True)], axis = 1)\n",
    "            break\n",
    "        R_emaildomain_dummies = pd.concat([R_emaildomain_dummies, \n",
    "                                           R_emaildomain.map(lambda x: 1 if x == colname else (x if pd.isnull(x) else 0))], \n",
    "                                          ignore_index=True, axis = 1)\n",
    "    R_emaildomain_dummies.columns = ['R_' + col for col in R_emaildomain_dummies_columns]\n",
    "\n",
    "    \n",
    "    tmp = df.loc[:, ['M1','M2','M3','M4','M6','M7','M9']]\n",
    "    M_dummies = pd.DataFrame()\n",
    "\n",
    "    for col in ['M1','M2','M3','M6','M9']:\n",
    "        M_dummies = pd.concat([M_dummies, tmp.loc[:, col].map(lambda x: 1 if x == 'T' else (x if pd.isnull(x) else 0))], ignore_index = True, axis = 1)\n",
    "    M_dummies = pd.concat([M_dummies, \n",
    "                           tmp.loc[:, 'M7'].map(lambda x: 1 if x == 'F' else (x if pd.isnull(x) else 0)),\n",
    "                           tmp.loc[:, 'M4'].map(lambda x: 1 if x == 'M2' else (x if pd.isnull(x) else 0))], \n",
    "                          ignore_index = True, axis = 1)    \n",
    "    M_dummies.columns = ['M1_T','M2_T','M3_T','M6_T','M9_T', 'M7_F', 'M4_M2']\n",
    "    \n",
    "    id_12_dummy = pd.get_dummies(df.id_12).loc[:,['NotFound']] # Not Found (21%)\n",
    "    id_15_dummies = pd.get_dummies(df.id_15).loc[:,['Found', 'New']] # Found, New (11%, 10%)\n",
    "    id_16_dummy = pd.get_dummies(df.id_16).loc[:, ['Found']] # Found (11%)\n",
    "    id_28_dummy = pd.get_dummies(df.id_28).loc[:, ['Found']] # Found (13%)\n",
    "    id_29_dummy = pd.get_dummies(df.id_29).loc[:, ['Found']] # Found (12%)\n",
    "    tmp = df.id_31.map(lambda x: x.split()[0].split('/')[0].lower() if type(x) == str else x)\n",
    "    id_31_dummy = pd.get_dummies(tmp).loc[:, ['chrome']] # chrome (13%)\n",
    "    id_35_dummy = pd.get_dummies(df.id_35).loc[:, ['F']] # 'F' (11%)\n",
    "    id_36_dummy = pd.get_dummies(df.id_36).loc[:, ['F']] # 'F' (22%)\n",
    "    id_37_dummy = pd.get_dummies(df.id_37).loc[:, ['T']] # 'T' (19%)\n",
    "    id_38_dummy = pd.get_dummies(df.id_38).loc[:, ['F']] # 'F' (13%)   \n",
    "    \n",
    "    id_dummies = pd.concat([id_12_dummy, id_15_dummies, id_16_dummy, id_28_dummy, id_29_dummy, id_31_dummy, id_35_dummy, id_36_dummy, id_37_dummy, id_38_dummy],\n",
    "                          ignore_index = True, axis = 1)\n",
    "    id_dummies.columns = ['id_12_Not_Found', 'id_15_Found', 'id_15_New', 'id_16_Found', 'id_28_Found', 'id_29_Found', 'id_31_chrome',\n",
    "                         'id_35_F', 'id_36_F', 'id_37_T', 'id_38_F']\n",
    "    \n",
    "    device_mobile = df.DeviceType.map(lambda x: 1 if x == \"mobile\" else (x if pd.isnull(x) else 0))\n",
    "    \n",
    "    df = pd.concat([df.loc[:, cols].reset_index(drop=True),\n",
    "                     product_dummies.reset_index(drop=True),\n",
    "                     card4_dummies.reset_index(drop=True), \n",
    "                     card_dummies.reset_index(drop=True), \n",
    "                     addr2_dummy.reset_index(drop=True), \n",
    "                     P_emaildomain_dummies.reset_index(drop=True),\n",
    "                    R_emaildomain_dummies.reset_index(drop=True),\n",
    "                   M_dummies.reset_index(drop=True),\n",
    "                   id_dummies.reset_index(drop=True),\n",
    "                   device_mobile.reset_index(drop=True)], axis = 1).copy()\n",
    "    \n",
    "    df.columns = list(cols) + list(product_dummies.columns) + list(card4_dummies.columns) + list(card_dummies.columns) + ['addr2_87'] + \\\n",
    "    list(P_emaildomain_dummies.columns) + list(R_emaildomain_dummies.columns) + list(M_dummies.columns) + list(id_dummies.columns) + ['device_mobile']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fbeta_recall_precision_acc(model_val_probs, actual_y, thresh_ps = np.linspace(.0,.99,1000), beta = 1.5):\n",
    "    \"\"\"\n",
    "    Plot Fbeta score, Recall, Precision, Accuracy across thresholds\n",
    "    Copied from the following notebook:\n",
    "    sf20_ds19/curriculum/project-03/class-imbalance/class_imbalance_instacart.ipynb\n",
    "    \n",
    "    args:\n",
    "    model_val_probs(np.array): predicted probabilities\n",
    "    actual_y(pd.DataFrame): actual y\n",
    "    clf: classifier/model\n",
    "    thresh_ps(np.array): candidate thresholds\n",
    "    beta(float): number of times that recall is more import than precision\n",
    "    \n",
    "    returns:\n",
    "    precs, recs, fbetas, acc_scores(tuple)\n",
    "    \"\"\"\n",
    "\n",
    "    thresh_ps = np.linspace(.0,.99,1000)\n",
    "\n",
    "    precs, recs, fbetas, acc_scores = [], [], [], []\n",
    "    for p in thresh_ps:\n",
    "        model_val_labels = model_val_probs >= p\n",
    "        prec, rec, fbeta, _ = precision_recall_fscore_support(actual_y, model_val_labels, beta = beta, average = 'binary')\n",
    "        precs.append(prec); recs.append(rec); fbetas.append(fbeta)\n",
    "        acc_scores.append(accuracy_score(actual_y, model_val_labels))\n",
    "\n",
    "    plt.plot(thresh_ps, fbetas)\n",
    "    plt.plot(thresh_ps, precs)\n",
    "    plt.plot(thresh_ps, recs)\n",
    "    plt.plot(thresh_ps, acc_scores)\n",
    "\n",
    "    plt.title('Metric Scores vs. Positive Class Decision Probability Threshold')\n",
    "    plt.legend(['Fbeta','Precision','Recall','Accuracy'])\n",
    "    plt.xlabel('P threshold')\n",
    "    plt.ylabel('Metric score')\n",
    "    plt.ylim(0, 1);\n",
    "\n",
    "    best_f1_score = np.max(fbetas) \n",
    "    best_thresh_p = thresh_ps[np.argmax(fbetas)]\n",
    "\n",
    "    print(f\"Best Fbeta({beta}) score {best_f1_score:8.5f} at prob decision threshold >= {best_thresh_p:8.5f}\")\n",
    "    \n",
    "    return precs, recs, fbetas, acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_min_samples_split(min_samples_split_list, scores):\n",
    "    \"\"\"\n",
    "    Plots the Precision, Recall, and Fbeta scores across multiple values of min_samples_split\n",
    "    \n",
    "    args:\n",
    "    min_samples_split_list(list): list of min_samples_split(integer)\n",
    "    scores(list): list of tuples with precision, recall, and fbeta score for each min_samples_split value\n",
    "    \n",
    "    returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    precs, recs, fbetas = [x[0] for x in scores], [x[1] for x in scores], [x[2] for x in scores]\n",
    "\n",
    "    plt.plot(min_samples_split_list, fbetas)\n",
    "    plt.plot(min_samples_split_list, precs)\n",
    "    plt.plot(min_samples_split_list, recs)\n",
    "\n",
    "    plt.title('Metric Scores vs. Positive Class Decision Probability Threshold')\n",
    "    plt.legend(['Fbeta','Precision','Recall'])\n",
    "    plt.xlabel('Min samples split')\n",
    "    plt.ylabel('Metric score')\n",
    "    plt.ylim(0, 1.1);\n",
    "\n",
    "    best_fbeta_score = np.max(fbetas) \n",
    "    best_min = min_samples_split_list[np.argmax(fbetas)]\n",
    "\n",
    "    print(f\"Best Fbeta(1.5) score {best_fbeta_score:8.5f} at min samples split = {best_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_max_features(max_features_list, scores):\n",
    "    \"\"\"\n",
    "    Plots the Precision, Recall, and Fbeta scores across multiple values of max_features\n",
    "    \n",
    "    args:\n",
    "    max_features_list(list): list of max_features(integer)\n",
    "    scores(list): list of tuples with precision, recall, and fbeta score for each max_features value\n",
    "    \n",
    "    returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    precs, recs, fbetas = [x[0] for x in scores], [x[1] for x in scores], [x[2] for x in scores]\n",
    "\n",
    "    plt.plot(max_features_list, fbetas)\n",
    "    plt.plot(max_features_list, precs)\n",
    "    plt.plot(max_features_list, recs)\n",
    "\n",
    "    plt.title('Metric Scores vs. Positive Class Decision Probability Threshold')\n",
    "    plt.legend(['Fbeta','Precision','Recall'])\n",
    "    plt.xlabel('Max features')\n",
    "    plt.ylabel('Metric score')\n",
    "    plt.ylim(0, 1.1);\n",
    "\n",
    "    best_fbeta_score = np.max(fbetas) \n",
    "    best_max_features = max_features_list[np.argmax(fbetas)]\n",
    "\n",
    "    print(f\"Best Fbeta(1.5) score {best_fbeta_score:8.5f} at max features = {best_max_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_n_estimators(n_estimators_list, scores):\n",
    "    \"\"\"\n",
    "    Plots the Precision, Recall, and Fbeta scores across multiple values of n_estimators(number of trees)\n",
    "    \n",
    "    args:\n",
    "    max_features_list(list): list of n_estimators(integer)\n",
    "    scores(list): list of tuples with precision, recall, and fbeta score for each n_estimators value\n",
    "    \n",
    "    returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    precs, recs, fbetas = [x[0] for x in scores], [x[1] for x in scores], [x[2] for x in scores]\n",
    "\n",
    "    plt.plot(n_estimators_list, fbetas)\n",
    "    plt.plot(n_estimators_list, precs)\n",
    "    plt.plot(n_estimators_list, recs)\n",
    "\n",
    "    plt.title('Metric Scores vs. Positive Class Decision Probability Threshold')\n",
    "    plt.legend(['Fbeta','Precision','Recall'])\n",
    "    plt.xlabel('Number of trees')\n",
    "    plt.ylabel('Metric score')\n",
    "    plt.ylim(0, 1.1);\n",
    "\n",
    "    best_fbeta_score = np.max(fbetas) \n",
    "    best_n_estimators = n_estimators_list[np.argmax(fbetas)]\n",
    "\n",
    "    print(f\"Best Fbeta(1.5) score {best_fbeta_score:8.5f} at n_estimators = {best_n_estimators}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest_result(clf, X_train, y_train, X_val, y_val, scores_train=[], scores_val=[]):\n",
    "    \"\"\"\n",
    "    \n",
    "    Prints Precision, Recall, and F beta scores with 0.5 cut-off probability and appends them to the input lists for training set and validation set.\n",
    "    \n",
    "    args:\n",
    "    clf(classifier): un-trained classifier with only hyperparameters set\n",
    "    X_train, y_train, X_val, y_val(pd.DataFrame): training set and validation set\n",
    "    scores_train(list): empty or non-empty list to append the evaluations on the training set\n",
    "    scores_val(list): empty or non-empty list to append the evaluations on the validation set\n",
    "    \n",
    "    returns:\n",
    "    clf, scores_train, scores_val (tuple)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    clf.fit(X_train, y_train);\n",
    "    prec, rec, fbeta, _ = precision_recall_fscore_support(y_train, clf.predict(X_train), beta = 1.5, average = 'binary')\n",
    "    scores_train.append((prec, rec, fbeta))\n",
    "    print(f\"Train set: Precision = {round(prec,5)}, Recall = {round(rec,5)}, F_1.5 = {round(fbeta,5)}\")\n",
    "    prec, rec, fbeta, _ = precision_recall_fscore_support(y_val, clf.predict(X_val), beta = 1.5, average = 'binary')\n",
    "    scores_val.append((prec, rec, fbeta))\n",
    "    print(f\"Val set: Precision = {round(prec,5)}, Recall = {round(rec,5)}, F_1.5 = {round(fbeta,5)}\\n\")\n",
    "    \n",
    "    return clf, scores_train, scores_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_trees_result(clf, X_train, y_train, X_val, y_val, scores_train=[], scores_val=[]):\n",
    "    \"\"\"\n",
    "    \n",
    "    Prints Precision, Recall, and F beta scores with 0.5 cut-off probability and appends them to the input lists for training set and validation set.\n",
    "    \n",
    "    args:\n",
    "    clf(classifier): un-trained classifier with only hyperparameters set\n",
    "    X_train, y_train, X_val, y_val(pd.DataFrame): training set and validation set\n",
    "    scores_train(list): empty or non-empty list to append the evaluations on the training set\n",
    "    scores_val(list): empty or non-empty list to append the evaluations on the validation set\n",
    "    \n",
    "    returns:\n",
    "    clf, scores_train, scores_val (tuple)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    clf.fit(X_train, y_train);\n",
    "    prec, rec, fbeta, _ = precision_recall_fscore_support(y_train, clf.predict(X_train), beta = 1.5, average = 'binary')\n",
    "    scores_train.append((prec, rec, fbeta))\n",
    "    print(f\"Train set: Precision = {round(prec,5)}, Recall = {round(rec,5)}, F_1.5 = {round(fbeta,5)}\")\n",
    "    prec, rec, fbeta, _ = precision_recall_fscore_support(y_val, clf.predict(X_val), beta = 1.5, average = 'binary')\n",
    "    scores_val.append((prec, rec, fbeta))\n",
    "    print(f\"Val set: Precision = {round(prec,5)}, Recall = {round(rec,5)}, F_1.5 = {round(fbeta,5)}\\n\")\n",
    "    \n",
    "    return clf, scores_train, scores_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_multicollinearity(df):\n",
    "    \"\"\"\n",
    "    Checks the multicollinearity among the features in the input dataframe.\n",
    "    \n",
    "    args:\n",
    "    df(pd.DataFrame): Given X matrix\n",
    "    \n",
    "    returns:\n",
    "    x_cor(pd.DataFrame): Dataframe with three columns: variable 1, variable 2, and their correlation\n",
    "    \"\"\"\n",
    "    c = df.corr().abs()\n",
    "    x_cor = c.unstack().sort_values(kind=\"quicksort\")\n",
    "    x_cor = pd.DataFrame(x_cor).reset_index(level=[0,1])\n",
    "    x_cor.columns = ['var1','var2','correlation']\n",
    "    x_cor = x_cor.loc[x_cor.var1 != x_cor.var2].sort_values(by='correlation', ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    return x_cor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
